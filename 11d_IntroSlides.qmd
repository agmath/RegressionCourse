---
title: "Multiple Linear Regression: Construction, Assessment, and Interpretation"
author: Dr. Gilbert
format: 
  revealjs:
    smaller: true
date: today
date-format: long
theme: serif
incremental: true
---

```{r global-options, include=FALSE}
library(tidyverse)
library(tidymodels)
library(palmerpenguins)
library(patchwork)
library(kableExtra)
tidymodels_prefer()

set.seed(123)

nobs <- 100
my_data <- tibble(
  x1 = runif(nobs, -2, 2),
  x2 = rnorm(nobs, 0, 1),
  x3 = runif(nobs, 0, 2*pi),
  y = exp(0.5*x1 + 0.4*x2^2 - 0.6*sin(x3) + rnorm(nobs, 0, 0.3))
    )

options(kable_styling_bootstrap_options = c("hover", "striped"))

#Set ggplot base theme
theme_set(theme_bw(base_size = 14))
```

```{css}
code.sourceCode {
  font-size: 1.3em;
  /* or try font-size: xx-large; */
}
```

## Motivation

. . . 

```{r}
p1 <- ggplot() + 
  geom_point(data = my_data,
             aes(x = x1, y = y),
             alpha = 0.5) + 
  labs(
    x = "x1",
    y = "y"
  )

p2 <- ggplot() + 
  geom_point(data = my_data,
             aes(x = x2, y = y),
             alpha = 0.5) + 
  labs(
    x = "x2",
    y = "y"
  )

p3 <- ggplot() + 
  geom_point(data = my_data,
             aes(x = x3, y = y),
             alpha = 0.5) +  
  labs(
    x = "x3",
    y = "y"
  )

(p1 + p2) / (p3) + plot_annotation(title = "A response (y) and three available predictors (x1, x2, and x3)")
```

. . . 

$$\mathbb{E}\left[y\right] = \beta_0 + \beta_1 x_1 + \beta_2 x_2 + \beta_3 x_3$$

## Motivation

$$\mathbb{E}\left[y\right] = \beta_0 + \beta_1 x_1 + \beta_2 x_2 + \beta_3 x_3$$

. . . 

```{r}
lr_spec <- linear_reg()
lr_rec <- recipe(y ~ ., data = my_data)
lr_wf <- workflow() %>%
  add_model(lr_spec) %>%
  add_recipe(lr_rec)

lr_fit <- lr_wf %>%
  fit(my_data)

new_data <- crossing(
  x1 = seq(-2, 2, by = 0.08),
  x2 = seq(-3, 3, by = 0.12),
  x3 = seq(0, 2*pi, by = 0.13)
)

new_data <- lr_fit %>%
  augment(new_data)

p1 <- ggplot() + 
  geom_line(data = new_data %>%
              filter(
                x2 %in% c(-1.44, 0.84, 2.16),
                x3 %in% c(1.69, 3.51, 5.33)
              ),
            aes(x = x1, y = .pred, color = as.factor(x2), linetype = as.factor(x3)),
            lwd = 1.25,
            show.legend = FALSE) +
  geom_point(data = my_data,
             aes(x = x1, y = y),
             alpha = 0.5) + 
  scale_color_manual(values = c("purple", "darkgreen")) + 
  labs(
    x = "x1",
    y = "y"
  )

p2 <- ggplot() + 
  geom_line(data = new_data %>%
              filter(
                x1 %in% c(-0.88, 0.56, 1.44),
                x3 %in% c(1.69, 3.51, 5.33)
              ),
            aes(x = x2, y = .pred, color = as.factor(x1), linetype = as.factor(x3)),
            lwd = 1.25,
            show.legend = FALSE) +
  geom_point(data = my_data,
             aes(x = x2, y = y),
             alpha = 0.5) + 
  scale_color_manual(values = c("purple", "darkgreen")) +
  labs(
    x = "x2",
    y = "y"
  )

p3 <- ggplot() + 
  geom_line(data = new_data %>% 
              filter(
                x1 %in% c(-0.88, 0.56, 1.44),
                x2 %in% c(-1.44, 0.84, 2.16)
              ),
            aes(x = x3, y = .pred, color = as.factor(x1), linetype = as.factor(x2)),
            lwd = 1.25,
            show.legend = FALSE) +
  geom_point(data = my_data,
             aes(x = x3, y = y),
             alpha = 0.5) +  
  scale_color_manual(values = c("purple", "darkgreen")) +
  labs(
    x = "x3",
    y = "y"
  )

(p1 + p2) / (p3) + plot_annotation(title = "Cross-sections of our model")
```

## Motivation

. . . 

Now that we have a fitted model, let's check out the *residuals*

. . .

```{r}
#| fig.height: 7

my_data <- lr_fit %>%
  augment(my_data) %>%
  mutate(residuals = y - .pred)

p1 <- my_data %>%
  ggplot() +
  geom_histogram(aes(x = residuals,
                     y = ..density..),
                 color = "black",
                 fill = "purple") +
  geom_density(aes(x = residuals),
               fill = "purple",
               alpha = 0.75)

p2 <- my_data %>%
  ggplot() +
  geom_point(aes(x = x1, y = residuals)) +
  geom_hline(yintercept = 0,
             color = "red",
             linetype = "dashed",
             lwd = 2)

p3 <- my_data %>%
  ggplot() +
  geom_point(aes(x = x2, y = residuals)) +
  geom_hline(yintercept = 0,
             color = "red",
             linetype = "dashed",
             lwd = 2)

p4 <- my_data %>%
  ggplot() +
  geom_point(aes(x = x3, y = residuals)) +
  geom_hline(yintercept = 0,
             color = "red",
             linetype = "dashed",
             lwd = 2)

p5 <- my_data %>%
  ggplot() +
  geom_point(aes(x = y, y = residuals)) +
  geom_hline(yintercept = 0,
             color = "red",
             linetype = "dashed",
             lwd = 2)

p6 <- my_data %>%
  ggplot() +
  geom_point(aes(x = .pred, y = residuals)) +
  geom_hline(yintercept = 0,
             color = "red",
             linetype = "dashed",
             lwd = 2)

(p1 + p2) / (p3 + p4) / (p5 + p6)
```

## Motivation

```{r}
(p1 + p2) / (p3 + p4) / (p5 + p6)
```

. . . 

The residual plots indicate some *badness*

+ Residuals are not normally distributed
+ Residuals don't seem to be random

  + Residuals show patterns with respect to some predictors and the response

+ Variance of residuals is not constant


## The Highlights

+ Types of residual plots

  + Plotting the distribution of residuals
  + Residuals versus the response
  + Residuals versus *fitted values* (model predictions)
  + Residuals versus predictors

+ Reactions to problematic residual plots
  
  + Transformations of the response variable
  + Transformations of the predictor(s)
  + ($\bigstar$ Later In Our Course $\bigstar$) Choosing a different model class
  
. . . 

**A Note on Structure:** This notebook will alternate between examining a particular type of residual plot and then identifying and executing a potential remedy for problematic results involving residuals.
  
## Types of Residual Plot: Distribution of Residuals

. . . 

**Purpose:** A plot of *the distribution of residuals* tells us whether our model results in prediction errors (residuals) that are normally distributed. 

. . . 

**Why Care?** Normal distribution of residuals is the assumption that allows us to build meaningful confidence- and prediction-intervals for the responses of new observations.

. . . 

```{r}
my_data %>%
  ggplot() +
  geom_histogram(aes(x = residuals, y = ..density..),
                 color = "black",
                 fill = "purple") + 
  geom_density(aes(x = residuals),
               fill = "purple",
               alpha = 0.75)
```

## Responding to Non-Normal Residuals

. . . 

**Remedies for Non-Normal Residuals:** Residuals being non-normally distributed is usually due to skew in the distribution of the response variable. We can model *transformations of the response* (ie. predicting $\ln\left(y\right)$, $\exp\left(y\right)$, $\sqrt{y}$, etc.) instead of directly modeling $y$.

. . . 

::::{.columns}

:::{.column width="50%"}

In the case of skewed residuals, like we see here, a reasonable approach is to model $\ln\left(y\right)$ rather than modeling $y$ directly.

:::

:::{.column width="50%}

```{r}
p1
```

:::

::::

. . . 

Let's make that change to our modeling strategy and revising the resulting residual plots.

## Updating Our Model (Transformed Response)

. . . 

As a reminder, in the residual plots below, we've chosen to model the logarithm of $y$ instead of modeling $y$ directly.

. . . 

```{r}
#| fig.height: 4

my_data <- my_data %>%
  select(-.pred, -residuals) %>%
  mutate(log_y = log(y))

lr_spec <- linear_reg()
lr_rec <- recipe(log_y ~ x1 + x2 + x3, data = my_data)
lr_wf <- workflow() %>%
  add_model(lr_spec) %>%
  add_recipe(lr_rec)

lr_fit <- lr_wf %>%
  fit(my_data)

my_data <- lr_fit %>%
  augment(my_data) %>%
  mutate(residuals = log_y - .pred)

p1 <- my_data %>%
  ggplot() +
  geom_histogram(aes(x = residuals,
                     y = ..density..),
                 color = "black",
                 fill = "purple") +
  geom_density(aes(x = residuals),
               fill = "purple",
               alpha = 0.75)

p2 <- my_data %>%
  ggplot() +
  geom_point(aes(x = x1, y = residuals)) +
  geom_hline(yintercept = 0,
             color = "red",
             linetype = "dashed",
             lwd = 2)

p3 <- my_data %>%
  ggplot() +
  geom_point(aes(x = x2, y = residuals)) +
  geom_hline(yintercept = 0,
             color = "red",
             linetype = "dashed",
             lwd = 2)

p4 <- my_data %>%
  ggplot() +
  geom_point(aes(x = x3, y = residuals)) +
  geom_hline(yintercept = 0,
             color = "red",
             linetype = "dashed",
             lwd = 2)

p5 <- my_data %>%
  ggplot() +
  geom_point(aes(x = log_y, y = residuals)) +
  geom_hline(yintercept = 0,
             color = "red",
             linetype = "dashed",
             lwd = 2)

p6 <- my_data %>%
  ggplot() +
  geom_point(aes(x = .pred, y = residuals)) +
  geom_hline(yintercept = 0,
             color = "red",
             linetype = "dashed",
             lwd = 2)

(p1 + p2) / (p3 + p4) / (p5 + p6)
```

. . . 

Certainly, this distribution of residuals is not perfectly normal, but it's better!

. . . 

We seem to have fixed the issue of non-constant variance with respect to model predictions (fitted values)

. . . 

...and improved, but not fixed, the issue of the association between residuals and the response.

## Types of Residual Plot: Residuals versus Predictors

. . . 

**Purpose:** A plot exploring potential associations between residuals and our utilized predictors tells us whether associations between the predictor and response are non-linear.

. . .

**Why Care:** We can adjust how we utilize our predictors in-model to obtain better predictive performance and descriptive properties.

. . . 

::::{.columns}

:::{.column width="60%"}

```{r}
#| fig.height: 8

p7 <- p2 + geom_smooth(aes(x1, y = residuals))
p8 <- p3 + geom_smooth(aes(x2, y = residuals)) 
p9 <- p4 + geom_smooth(aes(x3, y = residuals))

(p7/p8/p9)
```

:::

:::{.column width="40%"}

+ There seems to be no association between the residuals and `x1`
+ There is curvature in the association between the residuals and `x2` 
+ And curvature of a different type between the residuals and `x3`

:::

::::

## Responding to Associations Between Residuals and Predictors

. . . 

**Remedies for Associations Between Residuals and Predictors:** We can improve model fit (and perhaps explanatory value) by employing *transforms of predictors*.

. . . 

::::{.columns}

:::{.column width="60%"}

```{r}
#| fig.height: 7

(p7/p8/p9)
```

:::

:::{.column width="40%"}

+ No transformation of `x1` is necessary
+ The curved association between the residuals and `x2` indicates that perhaps a *quadratic* association between `x2` and $\ln\left(y\right)$ exists.
+ The curved association between the residuals and `x3` indicates that perhaps a *cubic* or *sinusoidal* association between `x3` and $\ln\left(y\right)$ exists.

:::

::::

. . . 

Again, we'll make these model updates (I'll show you how in the coming days) and revisit our residual plots.

## Updating Our Model (Transformed Predictors)

. . . 

As a reminder, in the residual plots below, we've chosen to model the logarithm of $y$ instead of modeling $y$ directly.

. . . 

```{r}
#| fig.height: 4

my_data <- my_data %>%
  select(-.pred, -residuals)

lr_spec <- linear_reg()
lr_rec <- recipe(log_y ~ x1 + x2 + x3, data = my_data) %>%
  step_poly(x2, degree = 2, options = list(raw = TRUE)) %>%
  step_mutate(x3 = sin(x3))
lr_wf <- workflow() %>%
  add_model(lr_spec) %>%
  add_recipe(lr_rec)

lr_fit <- lr_wf %>%
  fit(my_data)

my_data <- lr_fit %>%
  augment(my_data) %>%
  mutate(residuals = log_y - .pred)

p1 <- my_data %>%
  ggplot() +
  geom_histogram(aes(x = residuals,
                     y = ..density..),
                 color = "black",
                 fill = "purple") +
  geom_density(aes(x = residuals),
               fill = "purple",
               alpha = 0.75)

p2 <- my_data %>%
  ggplot() +
  geom_point(aes(x = x1, y = residuals)) +
  geom_hline(yintercept = 0,
             color = "red",
             linetype = "dashed",
             lwd = 2)

p3 <- my_data %>%
  ggplot() +
  geom_point(aes(x = x2, y = residuals)) +
  geom_hline(yintercept = 0,
             color = "red",
             linetype = "dashed",
             lwd = 2)

p4 <- my_data %>%
  ggplot() +
  geom_point(aes(x = x3, y = residuals)) +
  geom_hline(yintercept = 0,
             color = "red",
             linetype = "dashed",
             lwd = 2)

p5 <- my_data %>%
  ggplot() +
  geom_point(aes(x = log_y, y = residuals)) +
  geom_hline(yintercept = 0,
             color = "red",
             linetype = "dashed",
             lwd = 2)

p6 <- my_data %>%
  ggplot() +
  geom_point(aes(x = .pred, y = residuals)) +
  geom_hline(yintercept = 0,
             color = "red",
             linetype = "dashed",
             lwd = 2)

(p1 + p2) / (p3 + p4) / (p5 + p6)
```

. . . 

Again, nothing is *exactly perfect* in these residual plots, but...

## Updating Our Model (Transformed Predictors)

. . . 

```{r}
#| fig.height: 3.5

p2 <- p2 + geom_smooth(aes(x = x1, y = residuals))
p3 <- p3 + geom_smooth(aes(x = x2, y = residuals))
p4 <- p4 + geom_smooth(aes(x = x3, y = residuals))
p5 <- p5 + geom_smooth(aes(x = log_y, y = residuals))
p6 <- p6 + geom_smooth(aes(x = .pred, y = residuals))

(p1 + p2 + p3) / (p4 + p5 + p6)
```

+ The residuals are approximately normally distributed (with a spike near -0.3, which we should investigate)
+ The variance in residuals seems constant with respect to predictors, response, and predicted values

  + We can have more trust in our confidence and prediction intervals

+ There are no remaining associations between the residuals and the predictors

  + We are confident that we've squeezed out predictive value
  
## Summary

+ An analysis of residuals provides insight into model deficiencies
+ If residuals are not normally distributed, with a constant standard deviation, then we cannot trust our confidence- or prediction-intervals

  + In the case of non-normally distributed residuals or non-constant variance, we can try *transformations of the response variable*
  + This includes modeling the logarithm of the response, taking the exponential of the response, the square root of the response, etc.

+ If associations exist between the residuals and available predictors exist, then we've "left predictive power on the table"

  + We can explore *transformations of the predictors* in order to gain predictive accuracy and explanatory value
  + This can include raising predictors to a power, combining predictors together, taking trigonometric functions of predictors -- anything you can justify!
  
+ If associations exist between the residuals and either the response or the model's predicted values, then this means your model makes different errors depending on either the magnitude of the response or the magnitude of the predictions (ie. big response, big error / small response, small error)

  + In this case confidence- and prediction-intervals cannot be trusted
  + Exploring transformations of either the response or predictors can help here

## Next Time...

. . . 


<center>
<font size="120pt"><br/>

Categorical Predictors and Interpretations

</font>
</center>