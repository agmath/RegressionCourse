---
title: "Inclusion and Interpretation of Catgorical Predictors in Linear Regression Models"
author: Dr. Gilbert
format: revealjs
date: today
date-format: long
theme: serif
incremental: true
fontsize: 18pt
---

```{r global-options, include=FALSE}
library(tidyverse)
library(tidymodels)
library(palmerpenguins)
library(patchwork)
library(kableExtra)
tidymodels_prefer()

penguins <- palmerpenguins::penguins

penguins <- penguins %>%
  mutate(flipper_length_mm = as.numeric(flipper_length_mm),
         body_mass_g = as.numeric(body_mass_g))

set.seed(123)
penguin_splits <- initial_split(penguins)
penguins_train <- training(penguin_splits)
penguins_test <- testing(penguin_splits)

options(kable_styling_bootstrap_options = c("hover", "striped"))
options(scipen = 999)

#Set ggplot base theme
theme_set(theme_bw(base_size = 14))
```

```{css}
code.sourceCode {
  font-size: 1.3em;
  /* or try font-size: xx-large; */
}
```

## Motivation

```{r}
penguins_train %>%
  ggplot() + 
  geom_point(aes(x = bill_length_mm, y = body_mass_g)) + 
  labs(
    x = "Bill Length",
    y = "Body Mass"
  )
```

## Motivation

```{r}
#| fig-align: center

lin_reg_spec <- linear_reg() %>%
  set_engine("lm")

lin_reg_rec <- recipe(body_mass_g ~ bill_length_mm, data = penguins_train)

lin_reg_wf <- workflow() %>%
  add_model(lin_reg_spec) %>%
  add_recipe(lin_reg_rec)

lin_reg_fit <- lin_reg_wf %>%
  fit(penguins_train)

new_data <- tibble(
  bill_length_mm = seq(
    min(penguins_train$bill_length_mm, na.rm = TRUE),
    max(penguins_train$bill_length_mm, na.rm = TRUE),
    length.out = 250)
  )

new_data <- lin_reg_fit %>%
  augment(new_data)

ggplot() + 
  geom_point(data = penguins_train, 
             aes(x = bill_length_mm,
                 y = body_mass_g)) + 
  geom_line(data = new_data, 
            aes(x = bill_length_mm, 
                y = .pred),
            color = "blue",
            lwd = 1.5) + 
  labs(x = "Bill Length",
       y = "Body Mass")
```

```{r}
lin_reg_fit %>%
  glance() %>%
  kable() %>%
  kable_styling()

lin_reg_fit %>%
  extract_fit_engine() %>%
  tidy() %>%
  kable() %>%
  kable_styling()

```

## Motivation

```{r}
ggplot() + 
  geom_point(data = penguins_train, 
             aes(x = bill_length_mm,
                 y = body_mass_g,
                 color = species)) + 
  geom_line(data = new_data, 
            aes(x = bill_length_mm, 
                y = .pred),
            color = "blue",
            lwd = 1.5) + 
  labs(x = "Bill Length",
       y = "Body Mass")
```

## The Highlights

+ ANOVA as a linear regression model
+ Strategies for using categorical predictors in a model

  + Ordinal scorings
  + One-hot encodings versus dummy encodings

+ Feature engineering steps in recipes
+ Handling unknown or novel levels of a categorical variable
+ Fitting a model with categorical and numerical variables
+ Interpreting models with categorical predictors

## Playing Along

As always, you are encouraged to implement the ideas and techniques discussed here with your own data. You should...

1. Open RStudio and ensure that you are working within your `MAT300` project space
2. Open the notebook which includes your models for Air BnB rentals
3. Run all of the code in that notebook

. . . 

You'll add to that notebook here

## ANalysis Of VAriance

Does penguin body mass vary by species?

. . . 

$$\begin{array}{lcl} H_0 & : & \mu_{\text{Adelie}} = \mu_{\text{Chinstrap}} =  \mu_{\text{Gentoo}}\\
H_a & : & \text{At least one species has different average body mass}\end{array}$$

. . . 

```{r}
penguins_train %>%
  ggplot() + 
  geom_boxplot(aes(x = body_mass_g, 
                   y = species,
                   fill = species),
               show.legend = FALSE) + 
  labs(x = "Body Mass",
       y = "")
```

## ANalysis Of VAriance

Does penguin body mass vary by species?

$$\begin{array}{lcl} H_0 & : & \mu_{\text{Adelie}} = \mu_{\text{Chinstrap}} =  \mu_{\text{Gentoo}}\\
H_a & : & \text{At least one species has different average body mass}\end{array}$$

**ANOVA Test:**

```{r}
#| echo: true
#| eval: false
ANOVAtable <- aov(body_mass_g ~ species, data = penguins_train)

ANOVAtable %>%
  tidy()
```

```{r}
#| echo: false
#| eval: true
ANOVAtable <- aov(body_mass_g ~ species, data = penguins_train)

ANOVAtable %>%
  tidy() %>%
  kable() %>%
  kable_styling()
```

## ANalysis Of VAriance as Linear Regression

Does penguin body mass vary by species?

. . . 

$$\mathbb{E}\left[\text{body mass}\right] = \beta_0 + \beta_1\cdot\left(\text{???}\right)$$

. . . 

**Model-Based Test:**

```{r}
anova_spec <- linear_reg()

anova_rec <- recipe(body_mass_g ~ species, data = penguins_train) %>%
  step_dummy(species)

anova_wf <- workflow() %>%
  add_model(anova_spec) %>%
  add_recipe(anova_rec)

anova_fit <- anova_wf %>%
  fit(penguins_train)

anova_fit %>%
  glance() %>%
  kable() %>%
  kable_styling()
```

. . .

```{r}
anova_fit %>%
  extract_fit_engine() %>%
  tidy() %>%
  kable() %>%
  kable_styling()
```

## ANalysis Of VAriance as Linear Regression

Does penguin body mass vary by species?

$$\mathbb{E}\left[\text{body mass}\right] = \beta_0 + \beta_1\cdot\left(\text{speciesChinstrap}\right) + \beta_2\cdot\left(\text{speciesGentoo}\right)$$

**Model-Based Test:**

```{r}
anova_fit %>%
  glance() %>%
  kable() %>%
  kable_styling()
```

```{r}
anova_fit %>%
  extract_fit_engine() %>%
  tidy() %>%
  kable() %>%
  kable_styling()
```

## ANalysis Of VAriance as Linear Regression

Does penguin body mass vary by species?

$$\mathbb{E}\left[\text{body mass}\right] \approx 3703.95 - 18.45\cdot\left(\text{speciesChinstrap}\right) + 1298.23\cdot\left(\text{speciesGentoo}\right)$$

```{r}
#| fig.align: center

new_data <- tibble(
  species = as.factor(c("Adelie", "Chinstrap", "Gentoo"))
)

new_data <- anova_fit %>%
  augment(new_data) %>%
  bind_cols(
    anova_fit %>%
      predict(new_data, type = "conf_int")
  )

set.seed(123)

ggplot() + 
  geom_jitter(data = penguins_train,
              aes(x = as.numeric(species),
                  y = body_mass_g,
                  color = species), 
              width = 0.25, 
              height = 0) + 
  geom_segment(data = new_data,
             aes(x = as.numeric(species) - .25, xend = as.numeric(species) + 0.25, y = .pred, yend = .pred, color = species),
             lwd = 3,
             alpha = 0.75) + 
  scale_x_continuous(breaks = 1:length(levels(penguins_train$species)), 
                     labels = levels(penguins_train$species)) +
  theme(legend.position = "None") +
  labs(x = "",
       y = "Body Mass")
```

## ANalysis Of VAriance as Linear Regression

Does penguin body mass vary by species?

$$\mathbb{E}\left[\text{body mass}\right] \approx 3703.95 - 18.45\cdot\left(\text{speciesChinstrap}\right) + 1298.23\cdot\left(\text{speciesGentoo}\right)$$

```{r}
#| fig.align: center

new_data <- tibble(
  species = as.factor(c("Adelie", "Chinstrap", "Gentoo"))
)

new_data <- anova_fit %>%
  augment(new_data) %>%
  bind_cols(
    anova_fit %>%
      predict(new_data, type = "conf_int")
  )

set.seed(123)

ggplot() + 
  geom_rect(data = new_data,
              aes(xmin = as.numeric(species) - 0.25, 
                  ymin = .pred_lower, 
                  xmax = as.numeric(species) + 0.25,
                  ymax = .pred_upper,
                  color = species),
            alpha = 0.2) + 
  geom_jitter(data = penguins_train,
              aes(x = as.numeric(species),
                  y = body_mass_g,
                  color = species), 
              width = 0.25, 
              height = 0) + 
  geom_segment(data = new_data,
             aes(x = as.numeric(species) - .25, xend = as.numeric(species) + 0.25, y = .pred, yend = .pred, color = species),
             lwd = 3,
             alpha = 0.75) + 
  scale_x_continuous(breaks = 1:length(levels(penguins_train$species)), 
                     labels = levels(penguins_train$species)) +
  theme(legend.position = "None") +
  labs(x = "",
       y = "Body Mass")
```

<!-- 

## ANalysis Of VAriance as Linear Regression

<center>
<br/>
$\bigstar$ Let's try it! $\bigstar$

<br/>
</center>

1. Identify a categorical predictor of price in your Air BnB data set
2. Build a model using only that categorical predictor
3. Determine the result of your ANOVA test
4. Write down the estimated form of the model

-->

## Categorical Predictors in Models

Linear regression models depend on multiplication and addition

. . . 

These operations are not meaningful for categories (*Adelie*, *Torgersen*, *red*, etc.)

. . .

$$\mathbb{E}\left[\text{body mass}\right] = \beta_0 + \beta_1\cdot\left(\text{species}\right)$$

. . .

How would we evaluate this model for say, a Chinstrap penguin?

. . .

\begin{align} \mathbb{E}\left[\text{body mass}\right] &= \beta_0 + \beta_1\cdot\left(\text{Chinstrap}\right)\\
&= \text{???}
\end{align}

. . .

We need some way to convert categories into numeric quantities so that our models can consume them

## Strategies for Using Categorical Predictors (Scoring)

We can use an *ordinal* scoring method

. . .

<center>

Species | Score
---|---
Adelie | 1
Chinstrap | 2
Gentoo | 3

</center>

. . .

**Advantages:**

  + Only one $\beta$-coefficient required for inclusion of the categorical predictor
  + Can accommodate/model rankings between levels

. . . 

**Drawbacks:**

  + Imposes an ordering on categories
  + Enforces relationships between effect sizes
  
    + For example, the effect of being *Gentoo* on body mass is $3\times$ the effect of being *Adelie*
    + The expected difference in body mass between *Adelie* and *Chinstrap* penguins is the same as the expected difference between *Chinstrap* and *Gentoo* penguins
    
## Strategies for Using Categorical Predictors (Dummy Variables)

We can use *dummy* (or *indicator*) variables to encode the category

```{r}
penguins_train %>%
  head() %>%
  select(species) %>%
  mutate(
    speciesAdelie = ifelse(species == "Adelie", 1, 0),
    speciesChinstrap = ifelse(species == "Chinstrap", 1, 0),
    speciesGentoo = ifelse(species == "Gentoo", 1, 0)
  ) %>%
  kable() %>%
  kable_styling()
```

## Strategies for Using Categorical Predictors (Dummy Variables)

We can use *dummy* (or *indicator*) variables to encode the category

So we don't need the `species` variable any longer

```{r}
penguins_train %>%
  head() %>%
  select(species) %>%
  mutate(
    speciesAdelie = ifelse(species == "Adelie", 1, 0),
    speciesChinstrap = ifelse(species == "Chinstrap", 1, 0),
    speciesGentoo = ifelse(species == "Gentoo", 1, 0)
  ) %>%
  select(-species) %>%
  kable(caption = "A One-Hot Encoding") %>%
  kable_styling()
```

## Strategies for Using Categorical Predictors (Dummy Variables)

We can use *dummy* (or *indicator*) variables to encode the category

So we don't need the `species` variable any longer

And we don't need every *indicator* column, either

```{r}
penguins_train %>%
  head() %>%
  select(species) %>%
  mutate(
    speciesAdelie = ifelse(species == "Adelie", 1, 0),
    speciesChinstrap = ifelse(species == "Chinstrap", 1, 0),
    speciesGentoo = ifelse(species == "Gentoo", 1, 0)
  ) %>%
  select(-species, -speciesAdelie) %>%
  kable(caption = "A Dummy Encoding") %>%
  kable_styling()
```

## Strategies for Using Categorical Predictors (Dummy Variables)

We can use *dummy* (or *indicator*) variables to encode the category

So we don't need the `species` variable any longer

And we don't need every *indicator* column, either

```{r}
penguins_train %>%
  head() %>%
  select(species) %>%
  mutate(
    speciesAdelie = ifelse(species == "Adelie", 1, 0),
    speciesChinstrap = ifelse(species == "Chinstrap", 1, 0),
    speciesGentoo = ifelse(species == "Gentoo", 1, 0)
  ) %>%
  select(-speciesAdelie) %>%
  kable() %>%
  kable_styling()
```

. . .

**Advantage:** Can model variable effect sizes between levels

. . .

**Disadvantage:** Requires more $\beta$-coefficients (one less than the number of unique levels)

## Strategies for Using Categorical Predictors

<center>
<br/>
$\bigstar$ Let's try it! $\bigstar$

<br/>
</center>

1. Choose one categorical predictor of price for a rental in your dataset
2. How many levels of your categorical variable are there?
3. Is a scoring method meaningful for the levels of your variable? Why or why not?
4. What would a dummy-encoding of your categorical variable look like? How many indicator columns would be introduced?

## Back to Our Model from Earlier

```{r}
anova_fit %>%
  extract_fit_engine() %>%
  tidy() %>%
  kable() %>%
  kable_styling()
```

. . . 

$$\mathbb{E}\left[\text{body mass}\right] \approx 3703.95 - 18.45\cdot\left(\text{speciesChinstrap}\right) + 1398.23\cdot\left(\text{speciesGentoo}\right)$$

. . .

**Prediction for Adelie:** $3703.95 - 18.45\cdot\left(0\right) + 1398.23\cdot\left(0\right) \approx 3703.95\text{g}$

. . .

**Prediction for Chinstrap:** $3703.95 - 18.45\cdot\left(1\right) + 1398.23\cdot\left(0\right) \approx 3685.5\text{g}$

. . .

**Prediction for Gentoo:** $3703.95 - 18.45\cdot\left(0\right) + 1398.23\cdot\left(1\right) \approx 5102.18\text{g}$

. . . 

> **Note:** The interpretations for these $\beta$-coefficients on the species dummy variables are not as slopes. They are direct <u>adjustments</u> to the expected body mass, accounting for differences in species. The *intercept* in this model is the predicted response for the *base level*.

## Including Categorical Predictors with `{tidymodels}`

. . .

The act of transforming a categorical column into either ordinal scores or dummy variables is a *feature-engineering* step.

. . .

We'll encounter more feature engineering steps throughout our course. In the `{tidymodels}` framework, feature engineering steps are carried out as *steps* appended to our `recipe()`.

. . .

To build the linear regression version of our ANOVA test, I used the following to set up the model:

```{r}
#| echo: true
#| eval: false

anova_spec <- linear_reg() %>%
  set_engine("lm")

anova_rec <- recipe(body_mass_g ~ species, data = penguins_train) %>%
  step_dummy(species)
```

. . .

I then used our usual code to construct the *workflow* and *fit* the model to our training data.

## Including Categorical Predictors with `{tidymodels}`

<center>
<br/>
$\bigstar$ Let's try it! $\bigstar$

</center>

1. Add a new section to your notebook on the use of categorical predictors in models
2. Set up a linear regression model which uses your chosen categorical variable as the sole predictor of rental price

    + Create a model *specification*
    + Create a *recipe* and use the `step_dummy()` feature-engineering step in order to convert your categorical column into a collection of indicator columns
    + Package your *specification* and *recipe* together into a *workflow*
    + Fit the *workflow* to the *training data*

3. Examine the *global* and *term-based* model metrics
4. Write down the equation of the model
5. Determine the resulting model for several of the levels of your categorical variable

## Something More Interesting

Let's build a model that better corresponds to the plot we began this discussion with...

```{r}
ggplot() + 
  geom_point(data = penguins_train, 
             aes(x = bill_length_mm,
                 y = body_mass_g,
                 color = species)) + 
  labs(x = "Bill Length",
       y = "Body Mass")
```

. . . 

Can we accomodate both `bill_length_mm` (a numerical variable) and `species` (a categorical variable) into a single model?

## Bill Length and Species as Predictors of Body Mass

. . .

Start with our model *specification*

```{r}
#| echo: true
#| eval: true

lr_sbl_spec <- linear_reg() %>%
  set_engine("lm")
```

. . .

Now our *recipe*, with the feature engineering step included

```{r}
#| echo: true
#| eval: true

lr_sbl_rec <- recipe(body_mass_g ~ bill_length_mm + species, data = penguins_train) %>%
  step_dummy(species)
```

. . . 

Next, we package both the model and recipe together into a *workflow*

```{r}
#| echo: true
#| eval: true

lr_sbl_wf <- workflow() %>%
  add_model(lr_sbl_spec) %>%
  add_recipe(lr_sbl_rec)
```

. . .

And, finally, fit the workflow to the training data

```{r}
#| echo: true
#| eval: true

lr_sbl_fit <- lr_sbl_wf %>%
  fit(penguins_train)
```

## Models with Numerical and Categorical Predictors

<center>
<br/>
$\bigstar$ Let's try it! $\bigstar$

<br/>
</center>

1. Create an instance of a modeling *workflow* which uses at least one numerical predictor and at least one categorical predictor
2. Fit your workflow to your training data

## Assessing Our New Model

. . .

**Global Test for Model Utility:**

```{r}
#| echo: true
#| eval: false

lr_sbl_fit %>%
  glance()
```

```{r}
#| echo: false
#| eval: true

lr_sbl_fit %>%
  glance() %>%
  kable() %>%
  kable_styling()
```

. . .

**Individual Term-Based Tests:**

```{r}
#| echo: true
#| eval: false

lr_sbl_fit %>%
  extract_fit_engine() %>%
  tidy(conf.int = TRUE)
```

```{r}
#| echo: false
#| eval: true

lr_sbl_fit %>%
  extract_fit_engine() %>%
  tidy(conf.int = TRUE) %>%
  kable() %>%
  kable_styling()
```

## Assessing Our New Model

**Individual Term-Based Tests:**

```{r}
#| echo: true
#| eval: false

lr_sbl_fit %>%
  extract_fit_engine() %>%
  tidy(conf.int = TRUE)
```

```{r}
#| echo: false
#| eval: true

lr_sbl_fit %>%
  extract_fit_engine() %>%
  tidy(conf.int = TRUE) %>%
  kable() %>%
  kable_styling()
```

. . .

**Note:** Controlling for differences in bill length, all three species have statistically discernible average body masses

. . .

\begin{align} \mathbb{E}\left[\text{body mass}\right] \approx 83.47 ~+ &~93.83\cdot\left(\text{bill length}\right) - 966.14\cdot\left(\text{speciesChinstrap}\right) +\\ &~543.47\cdot\left(\text{speciesGentoo}\right)\end{align}

## An Improvement Over the Species-Only Model?

::::{.columns}

:::{.column width="50%"}

**Species-Only Model:**

```{r}
anova_fit %>%
  glance() %>%
  pivot_longer(cols = everything(),
               names_to = "metric",
               values_to = "value") %>%
  kable() %>%
  kable_styling()
```

:::

:::{.column width="50%"}

**Species and Bill Length Model:**

```{r}
lr_sbl_fit %>%
  glance() %>%
  pivot_longer(cols = everything(),
               names_to = "metric",
               values_to = "value") %>%
  kable() %>%
  kable_styling()
```

:::

::::

## Assessing and Comparing Models

<center>
<br/>
$\bigstar$ Let's try it! $\bigstar$

<br/>
</center>

1. Obtain and interpret your *global* and *term-based* model metrics
2. Compare your model's training metrics to those of your previous models
3. Obtain performance metrics for this new model using the *test* data. How does this model perform on unknown test observations compared to your previous models?

## Visualizing Model Coefficients

```{r}
#| echo: false
#| eval: true

lr_sbl_fit %>%
  extract_fit_engine() %>%
  tidy(conf.int = TRUE) %>%
  select(-statistic) %>%
  kable() %>%
  kable_styling()
```

. . .

::::{.columns}

:::{.column width="15%"}

:::

:::{.column width="70%"}

```{r}
lr_sbl_fit %>%
  extract_fit_engine() %>%
  tidy(conf.int = TRUE) %>%
  ggplot() + 
  geom_errorbarh(aes(xmin = conf.low, xmax = conf.high, y = term, color = term),
                 lwd = 1.25) + 
  geom_point(aes(x = estimate, y = term, color = term),
             size = 4) + 
  geom_vline(xintercept = 0, linetype = "dashed", lwd = 1.25) +
  labs(title = "Model Coefficients",
       x = "Coefficient",
       y = "") + 
  theme(legend.position = "None")
```

:::

:::{.column width="15%"}

:::

::::

## Visualizing Coefficients

<center>
<br/>
$\bigstar$ Let's try it! $\bigstar$

<br/>
</center>

1. Try to construct a plot which allows you to visualize your model coefficients and their plausible ranges.
2. What does the plot tell you about each model term?

## Interpreting this Model

```{r}
#| echo: false
#| eval: true

lr_sbl_fit %>%
  extract_fit_engine() %>%
  tidy() %>%
  kable() %>%
  kable_styling(font_size = 18)
```

\begin{align} \mathbb{E}\left[\text{body mass}\right] \approx 83.47 ~+ &~93.83\cdot\left(\text{bill length}\right) - 966.14\cdot\left(\text{speciesChinstrap}\right) +\\ &~543.47\cdot\left(\text{speciesGentoo}\right)\end{align}

+ (*Intercept*) Interpretation of this intercept is not meaningful because it would correspond to a penguin with a bill length of 0mm.

+ (*Bill Length*) Controlling for differences in species, we expect a 1mm increase in bill length to be associated with a 93.83g increase in body mass, on average.

+ (*Chinstrap*) Given a Chinstrap and Adelie penguin with the same bill length, we expect the Chinstrap to have a lower body mass by about 966.14g, on average.

+ (*Gentoo*) Given a Gentoo and Adelie penguin with the same bill length, we expect the Gentoo to have greater mass by about 543.47g, on average.

  + **Note:** Some intercept comparisons are misleading since Adelie bill lengths rarely overlap the other two species'

## Interpreting the Model

<center>
<br/>
$\bigstar$ Let's try it! $\bigstar$

<br/>
</center>

1. If you haven't done so already, write down your estimated model form.
2. Interpret the *intercept* (if appropriate) and the estimated effect of each *predictor* on the response.

## Making Predictions with this Model

```{r}
#| echo: true
#| eval: true
#| code-line-numbers: "|1-4|6-7|8-15|16-19|6-19|"

new_data <- crossing(
  bill_length_mm = seq(32.1, 59.6, length.out = 250),
  species = c("Adelie", "Chinstrap", "Gentoo")
)

new_data <- lr_sbl_fit %>%
  augment(new_data) %>%
  bind_cols(
    lr_sbl_fit %>%
      predict(new_data, type = "conf_int") %>%
      rename(
        .conf_lower = .pred_lower,
        .conf_upper = .pred_upper
      )
  ) %>%
  bind_cols(
    lr_sbl_fit %>%
      predict(new_data, type = "pred_int")
  )
```

## Making Predictions with this Model

```{r}
#| echo: true
#| eval: false
#| fig-align: center
#| code-line-numbers: "|1|2-4|5-7|8-10|11-13|2-13|14-17|18|19|"

ggplot() + 
  geom_ribbon(data = new_data, 
              aes(x = bill_length_mm, ymin = .pred_lower, ymax = .pred_upper),
              alpha = 0.25) +
  geom_ribbon(data = new_data,
              aes(x = bill_length_mm, ymin = .conf_lower, ymax = .conf_upper),
              alpha = 0.5) +
  geom_line(data = new_data,
            aes(x = bill_length_mm, y = .pred, color = species),
            lwd = 1.25) +
  geom_point(data = penguins_train,
             aes(x = bill_length_mm, y = body_mass_g, color = species),
             alpha = 0.5) + 
  labs(
    x = "Bill Length",
    y = "Body Mass"
  ) + 
  facet_wrap(~species, ncol = 3) + 
  theme(legend.position = "None")
  
```

## Making Predictions with this Model

```{r}
#| echo: false
#| eval: true
#| fig-align: center

ggplot() + 
  geom_ribbon(data = new_data, 
              aes(x = bill_length_mm, ymin = .pred_lower, ymax = .pred_upper),
              alpha = 0.25) +
  geom_ribbon(data = new_data,
              aes(x = bill_length_mm, ymin = .conf_lower, ymax = .conf_upper),
              alpha = 0.5) +
  geom_line(data = new_data,
            aes(x = bill_length_mm, y = .pred, color = species),
            lwd = 1.25) +
  geom_point(data = penguins_train,
             aes(x = bill_length_mm,
                 y = body_mass_g,
                 color = species),
             alpha = 0.5) + 
  labs(
    x = "Bill Length",
    y = "Body Mass"
  ) + 
  facet_wrap(~species, ncol = 3) + 
  theme(legend.position = "None")
  
```

. . . 

> **Note:** Including the categorical predictor has resulted in our model taking the form of these separate, parallel lines.

## Making and Visualizing Model Predictions

<center>
<br/>
$\bigstar$ Let's try it! $\bigstar$

<br/>
</center>

1. Create a *counterfactual* set of new observations using `crossing()`
2. Use your model to make predictions for those new observations

    + Include *confidence intervals* and *prediction intervals* as well

3. Create a graphical representation of your model's predictions, including confidence- and prediction-bands

## Additional Feature-Engineering Steps for Categorical Predictors

+ If we expect to encounter missing values, we can use `step_unknown()` to create a *level* for unknown values or we can use `step_impute_mode()` to fill in missing levels with the most frequently observed level
+ If we expect to encounter new levels, unknown to the model at training time, we can use `step_novel()`
+ If we expect to have levels which are infrequently observed, we can group those levels together into an "`other`" level with `step_other()`

## Summary

+ We can include categorical predictors in a model to differentiate predictions across the levels of that variable
+ Including categorical predictors in a model requires the use of an encoding scheme in order to convert levels to numerical values

  + *Scoring* assigns a distinct number to each category
  
    + Implement it by adding `step_ordinalscore()` as a feature engineering step in a *recipe*  
  
  + *Dummy encodings* replace a categorical predictor with a collection of binary (0/1) variables indicating the levels of the categorical predictor
  
    + We can use the dummy variable technique by adding `step_dummy()` as a feature engineering step in a *recipe*
  
  + In the case of rare levels, we may want to also include `step_other()`, `step_unknown()`, or `step_novel()` as recipe steps as well
+ A linear regression model including a single categorical variable as the sole predictor is equivalent to ANOVA
+ For a linear regression model with at least one numerical predictor, the result of the inclusion of a categorical variable in the model is a shift in *intercept* (a vertical shift) for each of the levels of the categorical predictor

## Next Time...

<center>
<br/>
Model-Building, Assessment, and Interpretation Workshop

<br/>
</center>