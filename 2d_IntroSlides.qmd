---
title: "Overview of Statistical Learning (and Competition Overview)"
author: Dr. Gilbert
format: revealjs
fontsize: 24pt
date: today
date-format: long
theme: serif
incremental: true
---

```{r global-options, include=FALSE}
library(tidyverse)
library(tidymodels)

theme_set(theme_bw(base_size = 20))
```

## Comment: Confirmatory versus Exploratory Workflows

<div style="font-size:18pt">

:::{.fragment}

There are two different circumstances from which we can come at statistics and data projects.

:::

+ **Exploratory Settings:** Where we don't yet have well-defined expectations or formal hypotheses generated about associations, patterns, or relationships in our data.
+ **Confirmatory Settings:** Where we have generated formal hypotheses or perhaps even officially pre-registered them prior to collecting any data.

:::{.fragment}

The scenario we are in dictates the workflow we must use in order to conduct valid inference.

:::

</div>

## Exploratory Settings and Workflows

<div style="font-size:18pt">

:::{.fragment}

In MAT300, we'll often be working with data from contexts in which we don't have deep subject-matter expertise.

:::

:::{.fragment}

Because of this, we'll take an initial *exploratory stance* when working with our data.

:::

+ We'll use exploratory data analysis (EDA) to uncover patterns, associations, and relationships that may exist in the data.
+ We'll use what we learn from that EDA to inform our modeling choices.

:::{.fragment}

Generating hypotheses and then testing those hypotheses on the same observations compromises the validity of inference (see: *snooping*, *fishing*, *p-hacking*, etc.).

:::

:::{.fragment}

For this reason, in our course, we'll split our data into a *training* (exploratory) set and at least one *validation* set.

:::

:::{.fragment}

We'll conduct exploratory data analyses, generate hypotheses, and train models on the *training* data.

:::

:::{.fragment}

We'll assess the performance of those models on the *validation* set(s).

:::

</div>

## Confirmatory Settings and Workflows 

<div style="font-size:18pt">

:::{.fragment}

It is also common to approach a statistics/data problem with pre-generated or pre-registered hypotheses.

:::

:::{.fragment}

These hypotheses are declared prior to any data collection and are justified by theory, prior experience, or justifiable expectations.

:::

:::{.fragment}

In these cases, the *training* and *validation* set approach is not necessary.

:::

:::{.fragment}

The investigator can simply proceed with modeling, model assessment, and interpretation using all of the available data.

:::

:::{.fragment}

They may not, however, change their hypotheses or adjust their model after fitting and analysing the one corresponding to their initial hypotheses and still treat the resulting inference as confirmatory.

:::

:::{.fragment}

**Sometimes Splitting is Still Necessary:** If model predictions are going to be used to inform decision-making, then data splitting is still necessary, even in the confirmatory setting. This is because predictive performance metrics generated from the data the model was trained on will be overly optimistic.

:::

:::{.fragment}

**Important Takeaway Point:** While we take an *exploratory* approach in MAT300, all of the model construction, model assessment (particularly significance testing), and model interpretation techniques we learn apply directly in confirmatory settings as well.

:::

</div>

## Statistical Learning in Pictures

. . . 

```{r}
#| echo: false
#| message: false

num_pts <- 15
set.seed(123)
x <- runif(num_pts, 0, 100)
y <- (x - 75)^2 + rnorm(num_pts, 0, 350)

ggplot() + 
  geom_point(aes(x = x, y = y),
             size = 3) +
  labs(title = "Our observed data",
       x = "x", y = "y")
  
```

## Statistical Learning in Pictures

```{r}
#| echo: false
#| message: false

num_pts <- 15
set.seed(123)
x <- runif(num_pts, 0, 100)
y <- (x - 75)^2 + rnorm(num_pts, 0, 350)

ggplot() + 
  geom_point(aes(x = x, y = y),
             size = 3) +
  labs(title = "Our observed data",
       x = "x", y = "y")
  
```

+ **Goal:** Build a model $\displaystyle{\mathbb{E}\left[y\right] = \beta_0 + \beta_1 x}$ to predict $y$, given $x$.
+ **Generalized Goal:** Build a model $\displaystyle{\mathbb{E}\left[y\right] = \beta_0 + \beta_1 x_1 + \beta_2 x_2 + \cdots + \beta_k x_k}$ to predict $y$ given features $x_1, \cdots, x_k$.

  + $\beta_i$'s are *parameters*, *learned* from training data.

## Statistical Learning in Pictures

. . .

```{r}
#| echo: false
#| message: false

x_vals <- seq(0, 100, length.out = 500)
y_vals <- -750 + 50*x_vals

ggplot() + 
  geom_point(aes(x = x, y = y),
             size = 3) +
  geom_line(aes(x = x_vals, 
                y = y_vals),
            color = "purple",
            lwd = 2,
            alpha = 0.75) + 
  labs(title = 'How "good" is the purple model?',
       x = "x", y = "y")
```

+ This model doesn't capture the general trend between our observed $x$ and $y$ pairs.

## Statistical Learning in Pictures

. . . 

```{r}
#| echo: false
#| message: false

x_vals <- seq(0, 100, length.out = 500)
y_vals <- 7000 - 60*x_vals

ggplot() + 
  geom_point(aes(x = x, y = y),
             size = 3) +
  geom_line(aes(x = x_vals, 
                y = y_vals),
            color = "purple",
            lwd = 2,
            alpha = 0.75) + 
  labs(title = 'Okay, how about this one?',
       x = "x", y = "y")
```

+ Better job capturing the general trend (*sort of*).
+ Larger $x$ values are associated with smaller $y$ values.

## Statistical Learning in Pictures

```{r}
#| echo: false
#| message: false

preds <- 7000 - 60*x

ggplot() + 
  geom_point(aes(x = x, y = y),
             size = 3) +
  geom_line(aes(x = x_vals, 
                y = y_vals),
            color = "purple",
            lwd = 2,
            alpha = 0.75) +
  labs(title = "Check out those prediction errors",
       x = "x", y = "y")
```

## Statistical Learning in Pictures

```{r}
#| echo: false
#| message: false

preds <- 7000 - 60*x

ggplot() + 
  geom_point(aes(x = x, y = y),
             size = 3) +
  geom_line(aes(x = x_vals, 
                y = y_vals),
            color = "purple",
            lwd = 2,
            alpha = 0.75) +
  geom_segment(aes(x = x, xend = x,
                   y = y, yend = preds),
               color = "red", 
               size = 3,
               alpha = 0.5) + 
  labs(title = "Check out those prediction errors",
       x = "x", y = "y",
       caption = "Reducible error remains")
```

+ Always predicting too high!

  + We should overpredict sometimes and underpredict others. The *average* error should be $0$.

## Statistical Learning in Pictures

```{r}
#| echo: false
#| message: false

my_data <- tibble(x = x, y = y)

lin_reg_spec <- linear_reg()
lin_reg_rec <- recipe(y ~ x, data = my_data)
lin_reg_wf <- workflow() %>%
  add_model(lin_reg_spec) %>%
  add_recipe(lin_reg_rec)
lin_reg_fit <- lin_reg_wf %>%
  fit(my_data)

coefs <- lin_reg_fit %>%
  extract_fit_engine() %>%
  tidy() %>%
  pull(estimate)

y_vals_lin <- coefs[1] + x_vals*coefs[2]

ggplot() + 
  geom_point(aes(x = x, y = y),
             size = 3) +
  geom_line(aes(x = x_vals, 
                y = y_vals_lin),
            color = "purple",
            lwd = 2,
            alpha = 0.75) +
  labs(title = "Getting better?",
       x = "x", y = "y")
```

## Statistical Learning in Pictures

```{r}
#| echo: false
#| message: false

preds <- coefs[1] + x*coefs[2]

ggplot() + 
  geom_point(aes(x = x, y = y),
             size = 3) +
  geom_line(aes(x = x_vals, 
                y = y_vals_lin),
            color = "purple",
            lwd = 2,
            alpha = 0.75) +
  geom_segment(aes(x = x, xend = x,
                   y = y, yend = preds),
               color = "red", 
               size = 3,
               alpha = 0.5) + 
  labs(title = "Getting better?",
       x = "x", y = "y",
       caption = "Remaining error is irreducible (at least with this model form)")
```

## Statistical Learning in Pictures

```{r}
#| echo: false
#| message: false

ggplot() + 
  geom_point(aes(x = x, y = y),
             size = 3) +
  geom_line(aes(x = x_vals, 
                y = y_vals_lin),
            color = "purple",
            lwd = 2,
            alpha = 0.75) +
  geom_segment(aes(x = x, xend = x,
                   y = y, yend = preds),
               color = "red", 
               size = 3,
               alpha = 0.5) + 
  labs(title = "Getting better?",
       x = "x", y = "y")
```

+ Capturing the general trend?

  + ...*mostly*

+ Balanced errors?

  + âœ“

## How it works

. . . 

In this case, we have $\mathbb{E}\left[y\right] = \beta_0 + \beta_1\cdot x$ and we find $\beta_0$ (intercept) and $\beta_1$ (slope) to minimize the quantity

. . . 

$$\sum_{i = 1}^{n}{\left(y_{\text{obs}_i} - y_{\text{pred}_i}\right)^2}$$

## How it works

In this case, we have $\mathbb{E}\left[y\right] = \beta_0 + \beta_1\cdot x$ and we find $\beta_0$ (intercept) and $\beta_1$ (slope) to minimize the quantity

$$\sum_{i = 1}^{n}{\left(y_{\text{obs}_i} - \left(\beta_0 + \beta_1\cdot x_{\text{obs}_i}\right)\right)^2}$$

+ Changing $\beta_0$ and/or $\beta_1$ will change this sum.

## How it works

```{r}
#| echo: false
#| message: false

resids <- y - preds

ggplot() + 
  geom_point(aes(x = x, y = y),
             size = 3) +
  geom_line(aes(x = x_vals, 
                y = y_vals_lin),
            color = "purple",
            lwd = 2,
            alpha = 0.75) +
  geom_rect(aes(xmin = x, xmax = ifelse(x > 50, x - resids*(100/15000), x + resids*(100/15000)),
                ymin = preds, ymax = preds + resids),
               color = "red",
               fill = "red",
               alpha = 0.25) + 
  scale_x_continuous(labels = NULL) + 
  scale_y_continuous(labels = NULL) + 
  labs(title = "Residual Sum of Squares",
       x = "x", y = "y",
       caption = "Note: Not to scale -- axis marks removed for visual purposes")
```

## Interpreting the Model

. . . 

::::{.columns}

:::{.column width="50%"}

```{r}
ggplot() + 
  geom_point(aes(x = x, y = y),
             size = 3) +
  geom_line(aes(x = x_vals, 
                y = y_vals_lin),
            color = "purple",
            lwd = 2,
            alpha = 0.75) +
  labs(title = "What does the model tell us?",
       x = "x", y = "y")
```

:::

:::{.column width="50%}

$\displaystyle{\mathbb{E}\left[y\right] = 3783.21 - 45.3\cdot x}$

:::

::::

+ The expected value of $y$ when $x = 0$ is $3783.21$.
+ As $x$ increases, we expect $y$ to decrease (on average).
+ Given a unit increase in $x$, we expect $y$ to decrease by about $45.3$ units.

## Interpreting the Model

::::{.columns}

:::{.column width="50%"}

```{r}
ggplot() + 
  geom_point(aes(x = x, y = y),
             size = 3) +
  geom_line(aes(x = x_vals, 
                y = y_vals_lin),
            color = "purple",
            lwd = 2,
            alpha = 0.75) +
  labs(title = "What does the model tell us?",
       x = "x", y = "y")
```

:::

:::{.column width="50%}

$\displaystyle{\mathbb{E}\left[y\right] = 3783.21 - 45.3\cdot x}$

:::

::::

:::{.nonincremental}

+ The <font color="purple">expected</font> value of $y$ when $x = 0$ is $3783.21$.
+ As $x$ increases, we <font color="purple">expect</font> $y$ to decrease (on average).
+ Given a unit increase in $x$, we <font color="purple">expect</font> $y$ to decrease by <font color="purple">about</font> $45.3$ units.

:::

. . .

**Approach to Model Interpretation:** In general, we'll interpret the intercept (when appropriate) and the <font color="purple">expected</font> effect of a unit change in each predictor on the response

## Can we find a *better* model? 

. . . 

```{r}
#| echo: false
#| message: false

my_data <- tibble(x = x, y = y)

poly_reg_spec <- linear_reg()
poly_reg_rec <- recipe(y ~ x, data = my_data) %>%
  step_poly(x, degree = 7)
poly_reg_wf <- workflow() %>%
  add_model(poly_reg_spec) %>%
  add_recipe(poly_reg_rec)
poly_reg_fit <- poly_reg_wf %>%
  fit(my_data)

y_vals_poly <- poly_reg_fit %>%
  predict(tibble(x = x_vals)) %>%
  pull(.pred)

coefs <- poly_reg_fit %>%
  extract_fit_engine() %>%
  tidy() %>%
  pull(estimate)

ggplot() + 
  geom_point(aes(x = x, y = y),
             size = 3) +
  geom_line(aes(x = x_vals, 
                y = y_vals_poly),
            color = "purple",
            lwd = 2,
            alpha = 0.75) +
  labs(title = "Wowzers!",
       x = "x", y = "y")
```

## Can we interpret this model?

```{r}
#| echo: false
#| message: false

ggplot() + 
  geom_point(aes(x = x, y = y),
             size = 3) +
  geom_line(aes(x = x_vals, 
                y = y_vals_poly),
            color = "purple",
            lwd = 2,
            alpha = 0.75) +
  labs(title = "What a great model!",
       x = "x", y = "y")
```

+ The equation is $\mathbb{E}\left[y\right] \approx 1202 - 4911x +3156x^2 + 784x^3 +\\ 409x^4 -215x^5 -7x^6 -516x^7$
+ No thanks...

## Do we expect this model to generalize well?

```{r}
#| echo: false
#| message: false

ggplot() + 
  geom_point(aes(x = x, y = y),
             size = 3) +
  geom_line(aes(x = x_vals, 
                y = y_vals_poly),
            color = "purple",
            lwd = 2,
            alpha = 0.75) +
  labs(title = "Great fit on past data",
       x = "x", y = "y")
```

## Do we expect this model to generalize well?

```{r}
#| echo: false
#| message: false

num_new <- 10
set.seed(456)
x_new <- c(runif(num_new, 0, 100), 100, 0)
y_new <- (x_new - 75)^2 + rnorm(num_new + 2, 0, 350)

ggplot() + 
  geom_point(aes(x = x, y = y),
             size = 3) +
  geom_line(aes(x = x_vals, 
                y = y_vals_poly),
            color = "purple",
            lwd = 2,
            alpha = 0.75) +
  geom_point(aes(x = x_new, y = y_new),
             color = "red",
             size = 3) +
  labs(title = "Oh no! Bigger (unexpectedly large) errors with new data",
       x = "x", y = "y")
```

+ Especially near $x = 0$ and $x = 100$

## Is there a happy medium?

. . . 

```{r}
#| echo: false
#| message: false

quad_reg_spec <- linear_reg()
quad_reg_rec <- recipe(y ~ x, data = my_data) %>%
  step_poly(x, degree = 2)
quad_reg_wf <- workflow() %>%
  add_model(quad_reg_spec) %>%
  add_recipe(quad_reg_rec)
quad_reg_fit <- quad_reg_wf %>%
  fit(my_data)

y_vals_quad <- quad_reg_fit %>%
  predict(tibble(x = x_vals)) %>%
  pull(.pred)

coefs <- quad_reg_fit %>%
  extract_fit_engine() %>%
  tidy() %>%
  pull(estimate)

ggplot() + 
  geom_point(aes(x = x, y = y),
             size = 3) +
  geom_line(aes(x = x_vals, 
                y = y_vals_quad),
            color = "purple",
            lwd = 2,
            alpha = 0.75) +
  labs(title = "Something less wiggly",
       x = "x", y = "y")
```

## Is there a happy medium?

```{r}
#| echo: false
#| message: false

ggplot() + 
  geom_point(aes(x = x, y = y),
             size = 3) +
  geom_line(aes(x = x_vals, 
                y = y_vals_quad),
            color = "purple",
            lwd = 2,
            alpha = 0.75) +
  geom_point(aes(x = x_new, y = y_new),
             color = "red",
             size = 3) + 
  labs(title = "Something less wiggly",
       x = "x", y = "y")
```

+ Fits old and new observations similarly well
+ Equation $\displaystyle{\mathbb{E}\left[y\right] \approx 1202 -4912x + 3156x^2}$

  + We'll be able to interpret this
  
## How do we know what model is *right*?

. . . 

::::{.columns}

:::{.column width="50%"}

```{r}
#| fig.height: 8

ggplot() + 
  geom_point(aes(x = x, y = y),
             size = 3) + 
  geom_line(aes(x = x_vals, y = y_vals_lin),
            color = "purple",
            lwd = 2,
            alpha = 0.9) + 
  geom_line(aes(x = x_vals, y = y_vals_quad),
            color = "darkgreen",
            lwd = 2,
            alpha = 0.9) + 
  geom_line(aes(x = x_vals, y = y_vals_poly),
            color = "orange",
            lwd = 2,
            alpha = 0.9) + 
  geom_point(aes(x = x_new, y = y_new),
             color = "red",
             size = 3) +
  labs(title = "All three models",
       x = "x",
       y = "y")
```

:::

:::{.column width="50%"}

+ The <font color="purple">purple</font> model is *too straight*
+ The <font color="orange">orange</font> model is *too wiggly*
+ The <font color="green">green</font> model is *just right*

:::

::::

## How do we know what model is *right*?

. . . 

::::{.columns}

:::{.column width="50%"}

```{r}
#| fig.height: 8

ggplot() + 
  geom_point(aes(x = x, y = y),
             size = 3) + 
  geom_line(aes(x = x_vals, y = y_vals_lin),
            color = "purple",
            lwd = 2,
            alpha = 0.3) + 
  geom_line(aes(x = x_vals, y = y_vals_quad),
            color = "darkgreen",
            lwd = 2) + 
  geom_line(aes(x = x_vals, y = y_vals_poly),
            color = "orange",
            lwd = 2,
            alpha = 0.3) + 
  geom_point(aes(x = x_new, y = y_new),
             color = "red",
             size = 3) +
  labs(title = "All three models",
       x = "x",
       y = "y")
```

:::

:::{.column width="50%"}

+ We don't want to wait for new data to know we are wrong.

  + Use some of our available data for training
  + And the rest for validation

:::

::::

## Okay, but our predictions are all wrong...literally!

::::{.columns}

:::{.column width="50%"}

```{r}
#| fig.height: 8

ggplot() + 
  geom_point(aes(x = x, y = y),
             size = 3,
             alpha = 0.25) + 
  geom_line(aes(x = x_vals, y = y_vals_quad),
            color = "darkgreen",
            lwd = 2) + 
  geom_point(aes(x = x_new, y = y_new),
             color = "red",
             size = 3) +
  labs(title = "Our best model",
       x = "x",
       y = "y")
```

:::

:::{.column width="50%"}

+ *All models are wrong, but some are useful*, George Box (1976)
+ Predictions will be wrong but, with some assumptions, they have value

:::

::::

## Necessary Assumptions

. . .

For model and predictions

+ Training data are random and representative of population.

  + Otherwise, we should not be modeling this way.

+ Residuals (prediction errors) are normally distributed with mean $\mu = 0$ and constant standard deviation $\sigma$.

  + Allows construction of confidence intervals around predictions (making our models *right*).

## Necessary Assumptions

. . .

For interpretations of coefficients (statistical learning / inference)

+ No multicollinearity (predictors aren't correlated with one another)
  
## Summary

+ Building models $\mathbb{E}\left[y\right] = \beta_0 + \beta_1 x_1 + \beta_2 x_2 + \cdots + \beta_k x_k$
+ Predicting a *numerical* response ($y$) given features ($x_i$)
+ Need data to build the models -- some for training, some for validation

  + The $\beta_i$'s are *parameters* whose values are *learned/estimated* from *training* data
+ Model predictions will be wrong
+ As long as standard deviation of residuals (prediction errors) is constant, we can build meaningful confidence intervals for predictions
+ Can interpret models to gain insight into relationships between predictor(s) and response

## Competition Information

+ Predict bicycle rental duration for a city bike share program

+ **Response:** `duration`
+ **Predictors:** You have access to nearly 40 explanatory variables which could be useful in predicting the duration of a rental. 

  + Date and time information
  + Origin of the rental
  + Station and city information
  + Weather-based features
  + Bicycle availability

## How this works

<div style="font-size:18pt">

+ Six assignments to guide you.
+ You'll take data provided by the city bike share program and build models.
+ Kaggle will assess the predictions generated by your models.
+ A live leaderboard will let you know approximately where you stand, using a portion of the competition data.
+ You'll talk with one other about modeling ideas and perfomance differences. 
+ People *might* choose share their strategies, while others *might not* -- this is a competition after all.

:::{.fragment}

**Clarification:** All work submitted on these competitions must reflect your own analytical thinking and modeling decisions.

+ It is acceptable to ask peers for advice and to share what they did, but direct model-based code-sharing is not permitted. 
+ Use of AI tools is limited to debugging or correcting errors in code that you have written.
+ Using AI to generate models, analysis, or interpretations is inconsistent with the goals of these assignments, and is prohibited.

:::

:::{.fragment}

**Connection to Debrief Meetings:** Your submitted competition work will serve as part of the foundation for our debrief meetings. In these meetings, you will be expected to explain and justify your modeling choices, performance results, and interpretations. The purpose of these discussions is to ensure that the submitted work accurately reflects your understanding.

:::

</div>

## Why?

+ Interest in homework assignments generally ends after they're turned in and graded.
+ The competition assignments and competitive environment ask you to iterate on previous work.
+ You'll almost surely be interested in what other people have done, especially if their models have performed better than yours.
+ You'll talk with one another about strategies and modeling choices.
+ You'll be motivated to improve your model even between assignments.

## What past students say

+ The competition is fun
+ It is motivating
+ I learned more because I wanted to place better in the competition
+ Talking with others about their models made me more confident in my understanding of course material

## What you are building

+ An [analytics report](https://agmath.github.io/RegressionCourse/WhatIsAnAnalyticsReport.html)
+ You'll be building models *and* (more importantly) writing about your modeling choices and the performance of your models
+ Six assignments -- each focusing on part(s) of the modeling process and analytics report
+ Prepares you for the final project, where you'll do all this over again on a data set you identified and care about

## Next Time...

+ Way fewer slides! ðŸ¤•
+ An introduction to R
+ Getting our hands dirty!

. . . 

**Homework:** Start [Competition Assignment 1](https://agmath.github.io/RegressionCourse/CA1_StatementOfPurpose.html) -- join the competition, read the details, download the data, and start writing a *Statement of Purpose*